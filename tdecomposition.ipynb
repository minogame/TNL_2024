{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "# np.random.seed(420)\n",
    "import jax.numpy as jnp\n",
    "import functools, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import sys\n",
    "module_path = '/home/chao/Pcode/TNL_2024/'\n",
    "# Add the directory to sys.path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tenmul7 import NeuroTN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_TT_adj_matrix(order, rank, dim_mode):\n",
    "    adjm = np.diag(np.full((order-1,),rank), 1)\n",
    "    adjm = adjm + adjm.transpose()\n",
    "    np.fill_diagonal(adjm, dim_mode)\n",
    "\n",
    "    return adjm\n",
    "\n",
    "def generate_TR_adj_matrix(order, rank, dim_mode):\n",
    "    adjm = np.diag(np.full((order-1,), rank), 1) if np.isscalar(rank) else np.diag(rank[:-1], 1)\n",
    "    adjm[0, order-1] = rank if np.isscalar(rank) else rank[-1]\n",
    "    adjm = adjm + adjm.transpose()\n",
    "    np.fill_diagonal(adjm, dim_mode)\n",
    "\n",
    "    return adjm\n",
    "\n",
    "def index_to_onehot(indices, num_class):\n",
    "    idx = np.asarray(indices) if isinstance(indices, list) else indices\n",
    "    \n",
    "    if idx.ndim != 2:\n",
    "        raise ValueError(\"indices must be a 2D list or array\")\n",
    "    \n",
    "    N, M = idx.shape\n",
    "\n",
    "    one_hot_encoded = np.zeros((N, M, num_class), dtype=float)\n",
    "\n",
    "    one_hot_encoded[np.arange(N)[:, None], np.arange(M), idx] = 1\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "order_tensor = 8 # Order of the tensor\n",
    "rank_tensor = 10 # Rank of the tensor\n",
    "dim_tensor = 2 # Mode dimension\n",
    "percentage_of_train = 0.7\n",
    "percentage_of_test = 0.3\n",
    "alpha = 0.5\n",
    "\n",
    "if percentage_of_train + percentage_of_test > 1:\n",
    "    raise ValueError('The total percentage should be less than 1')\n",
    "\n",
    "# Use NeuroTN to generate tensor\n",
    "adjm = generate_TT_adj_matrix(order_tensor,rank_tensor,dim_tensor)\n",
    "print('adjm_data:\\n', adjm)\n",
    "\n",
    "\n",
    "# Data generation \n",
    "output_dim = np.array([0] * (order_tensor-1)+[0])\n",
    "init_TN = functools.partial(np.random.normal, loc=0.0, scale=alpha/(np.sqrt(rank_tensor)))\n",
    "# init_TN = functools.partial(np.random.normal, loc=0.0, scale=1)\n",
    "DATA =  NeuroTN(adjm, output_dim, activation=lambda x:x, initializer = init_TN, core_mode=2)\n",
    "\n",
    "# siz_data, siz_cores = compression_ratio(adjm, output_dim)\n",
    "\n",
    "# print('siz_data:', siz_data)\n",
    "# print('siz_cores:', siz_cores)\n",
    "\n",
    "idx_data = [list(combo) for combo in itertools.product(range(dim_tensor), repeat=order_tensor)]\n",
    "idx_onehot = index_to_onehot(idx_data, num_class=dim_tensor)\n",
    "values = DATA.network_contraction(idx_onehot, return_contraction=True)\n",
    "\n",
    "permuted_idx = np.random.permutation(idx_onehot.shape[0])\n",
    "length_training = int(len(permuted_idx)*percentage_of_train)\n",
    "length_test = int(len(permuted_idx)*percentage_of_test)\n",
    "\n",
    "data_training = idx_onehot[permuted_idx[:length_training]]\n",
    "values_training = values[permuted_idx[:length_training]]\n",
    "data_test = idx_onehot[permuted_idx[length_training:(length_test+length_training)]]\n",
    "values_test = values[permuted_idx[length_training:(length_test+length_training)]]\n",
    "\n",
    "print(len(permuted_idx))\n",
    "print(length_training)\n",
    "\n",
    "print('data_test.shape', data_test.shape)\n",
    "print('values_test.shape', values_test.shape)\n",
    "print('Mean of data_train', jnp.mean(values_training))\n",
    "print('Variance of data_train', jnp.var(values_training))\n",
    "\n",
    "# hist, bin_edges = np.histogram(values_training, bins=50)\n",
    "\n",
    "# plt.hist(values_training, bins=10, edgecolor='black')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjm_decomp = generate_TT_adj_matrix(order_tensor,300,dim_tensor)\n",
    "output_decomp = output_dim\n",
    "\n",
    "print('adjm_decomp:\\n', adjm_decomp)\n",
    "\n",
    "print('========================')\n",
    "\n",
    "TN = NeuroTN(adjm_decomp, output_decomp, activation=lambda x:x, initializer = init_TN, core_mode=2)\n",
    "TN.target_shape = None\n",
    "\n",
    "\n",
    "\n",
    "size_data = idx_onehot.shape[0]\n",
    "batch_size = size_data\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# print(data_training.shape)\n",
    "# print(values_training.shape)\n",
    "# print(type(data_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "loss_training = []\n",
    "loss_test = []\n",
    "eign_values = []\n",
    "for epoch in range(100000):\n",
    "    loss_training.append(TN.iteration(learning_rate, data_training, values_training, verbose=True))\n",
    "    if epoch % 100 == 0:\n",
    "        ntk_tensor = TN.ntk(data_training, opt_path='optimal')\n",
    "        eign_value, eign_vector = jnp.linalg.eigh(ntk_tensor[0])\n",
    "        eign_values.append(eign_value)\n",
    "        clear_output(wait=True)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        for ev in eign_values:\n",
    "            plt.plot(jnp.log(ev/jnp.max(ev)))\n",
    "        plt.title('eignValue of NTK')\n",
    "\n",
    "        plt.subplot(2,2,3)\n",
    "        # print(eign_vector[:,-1].shape)\n",
    "        plt.plot(eign_vector[:,-1])\n",
    "        plt.title('1st eignVector')\n",
    "        \n",
    "\n",
    "        predicted = TN.network_contraction(data_test, return_contraction=True)\n",
    "        loss_test.append(np.mean(np.sum(np.square(predicted - values_test).reshape(predicted.shape[0],-1), axis=-1)))\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.plot(np.log(loss_training))\n",
    "        plt.title('Training loss (log)')\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.plot(np.log(loss_test))\n",
    "        plt.title('Test loss (log)')\n",
    "        plt.show()\n",
    "        print('Epoch: ', epoch, 'Training Loss: ', loss_training[-1], '; Testing Loss: ', loss_test[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
